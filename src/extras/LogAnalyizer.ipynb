{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bed_mesh_points(job_content, x_count, y_count):\n",
    "    \"\"\"Extract bed mesh points from job content.\"\"\"\n",
    "    points_start_str = \"save_config: set [bed_mesh default] points =\"\n",
    "    start_index = job_content.find(points_start_str)\n",
    "    \n",
    "    # If the starting string is not found, return an empty list\n",
    "    if start_index == -1:\n",
    "        return []\n",
    "    \n",
    "    # Start from the line after the identified string\n",
    "    lines = job_content[start_index:].split(\"\\n\")[1:x_count+1]  # +1 because the first line is the identified string itself\n",
    "    \n",
    "    # Extract points from each line\n",
    "    points = []\n",
    "    for line in lines:\n",
    "        points_row = list(map(float, line.strip().split(\", \")))\n",
    "        points.append(points_row)\n",
    "    \n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_count</th>\n",
       "      <th>y_count</th>\n",
       "      <th>min_x</th>\n",
       "      <th>max_x</th>\n",
       "      <th>min_y</th>\n",
       "      <th>max_y</th>\n",
       "      <th>points</th>\n",
       "      <th>Job ID</th>\n",
       "      <th>Stats Data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>[[0.04125, 0.0275, 0.0125, 0.005, -0.025, -0.0...</td>\n",
       "      <td>18192.6</td>\n",
       "      <td>gcodein  mcu_awake  mcu_task_avg  mcu_ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>[[0.04375, 0.0325, 0.01625, 0.00125, -0.02625,...</td>\n",
       "      <td>19849.8</td>\n",
       "      <td>gcodein  mcu_awake  mcu_task_avg  mcu_tas...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    x_count  y_count  min_x  max_x  min_y  max_y  \\\n",
       "0       NaN      NaN    NaN    NaN    NaN    NaN   \n",
       "1       NaN      NaN    NaN    NaN    NaN    NaN   \n",
       "2       NaN      NaN    NaN    NaN    NaN    NaN   \n",
       "3       NaN      NaN    NaN    NaN    NaN    NaN   \n",
       "4       NaN      NaN    NaN    NaN    NaN    NaN   \n",
       "5       NaN      NaN    NaN    NaN    NaN    NaN   \n",
       "6       NaN      NaN    NaN    NaN    NaN    NaN   \n",
       "7       NaN      NaN    NaN    NaN    NaN    NaN   \n",
       "8       NaN      NaN    NaN    NaN    NaN    NaN   \n",
       "9       NaN      NaN    NaN    NaN    NaN    NaN   \n",
       "10      NaN      NaN    NaN    NaN    NaN    NaN   \n",
       "11      NaN      NaN    NaN    NaN    NaN    NaN   \n",
       "12      NaN      NaN    NaN    NaN    NaN    NaN   \n",
       "13      NaN      NaN    NaN    NaN    NaN    NaN   \n",
       "14      6.0      6.0   10.0  290.0   45.0  270.0   \n",
       "15      6.0      6.0   10.0  290.0   45.0  270.0   \n",
       "\n",
       "                                               points   Job ID  \\\n",
       "0                                                 NaN      NaN   \n",
       "1                                                 NaN      NaN   \n",
       "2                                                 NaN      NaN   \n",
       "3                                                 NaN      NaN   \n",
       "4                                                 NaN      NaN   \n",
       "5                                                 NaN      NaN   \n",
       "6                                                 NaN      NaN   \n",
       "7                                                 NaN      NaN   \n",
       "8                                                 NaN      NaN   \n",
       "9                                                 NaN      NaN   \n",
       "10                                                NaN      NaN   \n",
       "11                                                NaN      NaN   \n",
       "12                                                NaN      NaN   \n",
       "13                                                NaN      NaN   \n",
       "14  [[0.04125, 0.0275, 0.0125, 0.005, -0.025, -0.0...  18192.6   \n",
       "15  [[0.04375, 0.0325, 0.01625, 0.00125, -0.02625,...  19849.8   \n",
       "\n",
       "                                           Stats Data  \n",
       "0                                                 NaN  \n",
       "1                                                 NaN  \n",
       "2                                                 NaN  \n",
       "3                                                 NaN  \n",
       "4                                                 NaN  \n",
       "5                                                 NaN  \n",
       "6                                                 NaN  \n",
       "7                                                 NaN  \n",
       "8                                                 NaN  \n",
       "9                                                 NaN  \n",
       "10                                                NaN  \n",
       "11                                                NaN  \n",
       "12                                                NaN  \n",
       "13                                                NaN  \n",
       "14        gcodein  mcu_awake  mcu_task_avg  mcu_ta...  \n",
       "15       gcodein  mcu_awake  mcu_task_avg  mcu_tas...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def parse_stats_line_v3(stats_line):\n",
    "    \"\"\"Extract details from a single 'Stats' line.\"\"\"\n",
    "    patterns = {\n",
    "        'gcodein': r\"gcodein=(\\d+)\",\n",
    "        'mcu_awake': r\"mcu_awake=([\\d.]+)\",\n",
    "        'mcu_task_avg': r\"mcu_task_avg=([\\d.]+)\",\n",
    "        'mcu_task_stddev': r\"mcu_task_stddev=([\\d.]+)\",\n",
    "        'bytes_write': r\"bytes_write=(\\d+)\",\n",
    "        'bytes_read': r\"bytes_read=(\\d+)\",\n",
    "        'bytes_retransmit': r\"bytes_retransmit=(\\d+)\",\n",
    "        'bytes_invalid': r\"bytes_invalid=(\\d+)\",\n",
    "        'send_seq': r\"send_seq=(\\d+)\",\n",
    "        'receive_seq': r\"receive_seq=(\\d+)\",\n",
    "        'retransmit_seq': r\"retransmit_seq=(\\d+)\",\n",
    "        'srtt': r\"srtt=([\\d.]+)\",\n",
    "        'rttvar': r\"rttvar=([\\d.]+)\",\n",
    "        'rto': r\"rto=([\\d.]+)\",\n",
    "        'ready_bytes': r\"ready_bytes=(\\d+)\",\n",
    "        'upcoming_bytes': r\"upcoming_bytes=(\\d+)\",\n",
    "        'freq': r\"freq=(\\d+)\",\n",
    "        'sd_pos': r\"sd_pos=(\\d+)\",\n",
    "        'heater_bed_target': r\"heater_bed: target=(\\d+)\",\n",
    "        'temp': r\"temp=([\\d.]+)\",\n",
    "        'pwm': r\"pwm=([\\d.]+)\",\n",
    "        'sysload': r\"sysload=([\\d.]+)\",\n",
    "        'cputime': r\"cputime=([\\d.]+)\",\n",
    "        'memavail': r\"memavail=(\\d+)\",\n",
    "        'print_time': r\"print_time=([\\d.]+)\",\n",
    "        'buffer_time': r\"buffer_time=([\\d.]+)\"\n",
    "        }\n",
    "    parsed_details = {}\n",
    "    for detail, pattern in patterns.items():\n",
    "        match = re.search(pattern, stats_line)\n",
    "        if match:\n",
    "            # Determine if the value is a float or integer based on the key\n",
    "            if any(sub_str in detail for sub_str in ['temp', 'pwm', 'sysload', 'cputime', 'print_time', 'buffer_time', 'mcu_awake', 'mcu_task_avg', 'mcu_task_stddev', 'srtt', 'rttvar', 'rto']):\n",
    "                parsed_details[detail] = float(match.group(1))\n",
    "            else:\n",
    "                parsed_details[detail] = int(match.group(1))\n",
    "    return parsed_details\n",
    "\n",
    "def extract_job_details_improved(job_content):\n",
    "    \"\"\"Extract key details from a job's content using the improved points extraction.\"\"\"\n",
    "    patterns = {\n",
    "        'x_count': r\"x_count = (\\d+)\",\n",
    "        'y_count': r\"y_count = (\\d+)\",\n",
    "        'min_x': r\"min_x = ([\\d.]+)\",\n",
    "        'max_x': r\"max_x = ([\\d.]+)\",\n",
    "        'min_y': r\"min_y = ([\\d.]+)\",\n",
    "        'max_y': r\"max_y = ([\\d.]+)\"\n",
    "    }\n",
    "    details = {}\n",
    "    for detail, pattern in patterns.items():\n",
    "        detail_match = re.search(pattern, job_content)\n",
    "        if detail_match:\n",
    "            # Determine if the value is a float or integer\n",
    "            if any(sub_str in detail for sub_str in ['min_x', 'max_x', 'min_y', 'max_y']):\n",
    "                details[detail] = float(detail_match.group(1))\n",
    "            else:\n",
    "                details[detail] = int(detail_match.group(1))\n",
    "    \n",
    "    # Extract bed mesh points using the improved function\n",
    "    x_count = details.get('x_count', 0)\n",
    "    y_count = details.get('y_count', 0)\n",
    "    details['points'] = extract_bed_mesh_points(job_content, x_count, y_count)\n",
    "    \n",
    "    return details\n",
    "\n",
    "def parse_detailed_log_to_dataframe_v5(log_file_path):\n",
    "    with open(log_file_path, \"r\") as file:\n",
    "        content = file.read()\n",
    "\n",
    "    job_start_marker = \"Starting SD card print (position\"\n",
    "    job_end_marker = \"Exiting SD card print (position\"\n",
    "\n",
    "    jobs = []\n",
    "    job_parts = content.split(job_start_marker)\n",
    "    for job in job_parts[1:]:\n",
    "        job_content = job.split(job_end_marker)[0]\n",
    "        jobs.append(job_start_marker + job_content)\n",
    "\n",
    "    all_job_details = []\n",
    "    for job in jobs:\n",
    "        job_details = {}\n",
    "        if 'Mesh Bed Leveling Complete' in job:\n",
    "            job_details = extract_job_details_improved(job)\n",
    "            \n",
    "            stats_pattern = r\"Stats (\\d+\\.\\d+):\"\n",
    "            stats_matches = re.findall(stats_pattern, job)\n",
    "            if stats_matches:\n",
    "                job_details['Job ID'] = stats_matches[0]\n",
    "                stats_details = []\n",
    "                for stats_match in stats_matches:\n",
    "                    line_start = job.find(f\"Stats {stats_match}:\")\n",
    "                    line_end = job.find(\"\\n\", line_start)\n",
    "                    stats_line = job[line_start:line_end]\n",
    "                    stats_details.append(parse_stats_line_v3(stats_line))\n",
    "                job_details['Stats Data'] = pd.DataFrame(stats_details)\n",
    "        all_job_details.append(job_details)\n",
    "\n",
    "    df = pd.DataFrame(all_job_details)\n",
    "    return df\n",
    "\n",
    "# Example Usage\n",
    "detailed_df_v5 = parse_detailed_log_to_dataframe_v5(\"klippy.log\")\n",
    "detailed_df_v5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_data_to_csv(log_file_path, bed_mesh_output, stats_data_output):\n",
    "    \"\"\"\n",
    "    Parses the log file, extracts relevant data, and exports to two separate CSV files.\n",
    "    \n",
    "    Parameters:\n",
    "    - log_file_path: Path to the log file to be parsed.\n",
    "    - bed_mesh_output: Path where the Bed Mesh Leveling Details CSV will be saved.\n",
    "    - stats_data_output: Path where the Stats Data CSV will be saved.\n",
    "    \"\"\"\n",
    "    # Parse the log file\n",
    "    detailed_df = parse_detailed_log_to_dataframe_v5(log_file_path)\n",
    "    \n",
    "    # Export Bed Mesh Leveling Details\n",
    "    expected_columns = ['x_count', 'y_count', 'min_x', 'max_x', 'min_y', 'max_y', 'points']\n",
    "    columns_to_extract = [col for col in expected_columns if col in detailed_df.columns]\n",
    "    bed_mesh_df = detailed_df[columns_to_extract].copy()\n",
    "    bed_mesh_df.to_csv(bed_mesh_output, index=False)\n",
    "    \n",
    "    # Export Stats Data\n",
    "    all_stats_data = pd.DataFrame()\n",
    "    for idx, row in detailed_df.iterrows():\n",
    "        if isinstance(row['Stats Data'], pd.DataFrame):\n",
    "            stats_data = row['Stats Data'].copy()\n",
    "            stats_data['Job ID'] = row['Job ID']\n",
    "            all_stats_data = pd.concat([all_stats_data, stats_data], ignore_index=True)\n",
    "    all_stats_data.to_csv(stats_data_output, index=False)\n",
    "\n",
    "# Run the combined function\n",
    "bed_mesh_output_path = \"bed_mesh_details.csv\"\n",
    "stats_data_output_path = \"stats_data.csv\"\n",
    "export_data_to_csv(\"klippy.log\", bed_mesh_output_path, stats_data_output_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jobs_export'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def export_jobs_to_separate_csvs(log_file_path, output_directory):\n",
    "    \"\"\"\n",
    "    Parses the log file, extracts relevant data, and exports each job's details to separate CSV files.\n",
    "    \n",
    "    Parameters:\n",
    "    - log_file_path: Path to the log file to be parsed.\n",
    "    - output_directory: Directory where the CSV files will be saved.\n",
    "    \"\"\"\n",
    "    # Ensure output directory exists\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "    \n",
    "    # Parse the log file\n",
    "    detailed_df = parse_detailed_log_to_dataframe_v5(log_file_path)\n",
    "    \n",
    "    # Export each job's details to separate CSV files\n",
    "    for idx, row in detailed_df.iterrows():\n",
    "        # Exporting Bed Mesh Leveling Details\n",
    "        if not pd.isna(row['x_count']):\n",
    "            bed_mesh_filename = os.path.join(output_directory, f\"BedMesh_Job_{row['Job ID']}.csv\")\n",
    "            bed_mesh_data = {key: row[key] for key in ['x_count', 'y_count', 'min_x', 'max_x', 'min_y', 'max_y', 'points']}\n",
    "            bed_mesh_df = pd.DataFrame([bed_mesh_data])\n",
    "            bed_mesh_df.to_csv(bed_mesh_filename, index=False)\n",
    "        \n",
    "        # Exporting Stats Data\n",
    "        if isinstance(row['Stats Data'], pd.DataFrame):\n",
    "            stats_filename = os.path.join(output_directory, f\"Job_{row['Job ID']}.csv\")\n",
    "            row['Stats Data'].to_csv(stats_filename, index=False)\n",
    "\n",
    "# Run the function to export the jobs to separate CSV files\n",
    "output_dir = \"jobs_export\"\n",
    "export_jobs_to_separate_csvs(\"klippy.log\", output_dir)\n",
    "\n",
    "output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
